# main.py - Ø¨Ø§Øª ØªÙ„Ú¯Ø±Ø§Ù… Ø§Ø®Ø¨Ø§Ø± Ù…Ø¯ Ùˆ ÙØ´Ù† Ø§ÛŒØ±Ø§Ù†ÛŒ
# Ù†Ø³Ø®Ù‡ Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø¯ÙˆÙ† ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒ Ø¨Ù‡ SDK appwrite (Ø¨Ø§ requests Ø®Ø§Ù…)
# ÙÙ‚Ø· Û± Ù¾Ø³Øª Ø¯Ø± Ù‡Ø± Ø§Ø¬Ø±Ø§
# ÙÛŒÙ„ØªØ± Ø³Ø§Ø¯Ù‡ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø¯ Ùˆ ÙØ´Ù†
# Ø¹Ú©Ø³ Ø§Ø² RSS ÛŒØ§ og:image ØµÙØ­Ù‡ Ø®Ø¨Ø±
# Ú†Ú© ØªÚ©Ø±Ø§Ø±ÛŒ Ø¨Ø§ Ù„ÛŒÙ†Ú© Ùˆ hash Ù…Ø­ØªÙˆØ§
# Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Appwrite Ø¨Ø§ requests Ø®Ø§Ù…

import os
import asyncio
import feedparser
import requests
import hashlib
from datetime import datetime, timedelta, timezone
from telegram import Bot, LinkPreviewOptions
from bs4 import BeautifulSoup

async def main(event=None, context=None):
    print("[INFO] Ø´Ø±ÙˆØ¹ Ø¨Ø§Øª Ù…Ø¯ Ùˆ ÙØ´Ù†")

    token = os.environ.get('TELEGRAM_BOT_TOKEN')
    chat_id = os.environ.get('TELEGRAM_CHANNEL_ID')
    endpoint = os.environ.get('APPWRITE_ENDPOINT', 'https://cloud.appwrite.io/v1')
    project = os.environ.get('APPWRITE_PROJECT_ID')
    key = os.environ.get('APPWRITE_API_KEY')
    database_id = os.environ.get('APPWRITE_DATABASE_ID')
    collection_id = 'history'

    if not all([token, chat_id, endpoint, project, key, database_id]):
        print("[ERROR] Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ø­ÛŒØ·ÛŒ Ù†Ø§Ù‚Øµ")
        return {"status": "error"}

    bot = Bot(token=token)

    headers = {
        'Content-Type': 'application/json',
        'X-Appwrite-Project': project,
        'X-Appwrite-Key': key,
    }

    # Û²Û° ÙÛŒØ¯ ØªØ®ØµØµÛŒ Ù…Ø¯ØŒ ÙØ´Ù†ØŒ Ø²ÛŒØ¨Ø§ÛŒÛŒ Ùˆ Ø§Ø³ØªØ§ÛŒÙ„ Ø§ÛŒØ±Ø§Ù†ÛŒ
    rss_feeds = [
        "https://medopia.ir/feed/",
        "https://www.digistyle.com/mag/feed/",
        "https://www.chibepoosham.com/feed/",
        "https://www.tarahanelebas.com/feed/",
        "https://www.persianpood.com/feed/",
        "https://www.jument.style/feed/",
        "https://www.zibamoon.com/feed/",
        "https://www.sarak-co.com/feed/",
        "https://www.elsana.com/feed/",
        "https://www.beytoote.com/rss/fashion",
        "https://www.namnak.com/rss/fashion",
        "https://www.modetstyle.com/feed/",
        "https://www.antikstyle.com/feed/",
        "https://www.rnsfashion.com/feed/",
        "https://www.pattonjameh.com/feed/",
        "https://www.tonikaco.com/feed/",
        "https://www.zoomit.ir/feed/category/fashion-beauty/",
        "https://www.khabaronline.ir/rss/category/Ù…Ø¯-Ø²ÛŒØ¨Ø§ÛŒÛŒ",
        "https://fararu.com/rss/category/Ù…Ø¯-Ø²ÛŒØ¨Ø§ÛŒÛŒ",
        "https://www.digikala.com/mag/feed/?category=Ù…Ø¯-Ùˆ-Ø²ÛŒØ¨Ø§ÛŒÛŒ",
    ]

    now = datetime.now(timezone.utc)
    time_threshold = now - timedelta(hours=24)

    posted = False

    for url in rss_feeds:
        if posted:
            break

        try:
            feed = feedparser.parse(url)
            if not feed.entries:
                print(f"[INFO] ÙÛŒØ¯ Ø®Ø§Ù„ÛŒ: {url}")
                continue

            for entry in feed.entries:
                if posted:
                    break

                published = entry.get('published_parsed') or entry.get('updated_parsed')
                if not published:
                    continue

                pub_date = datetime(*published[:6], tzinfo=timezone.utc)
                if pub_date < time_threshold:
                    continue

                title = (entry.title or "").strip()
                link = (entry.link or "").strip()
                if not title or not link:
                    continue

                description = (entry.get('summary') or entry.get('description') or "").strip()

                # ÙÛŒÙ„ØªØ± Ø³Ø§Ø¯Ù‡ Ù…Ø¯ Ùˆ ÙØ´Ù† (Ø¨Ø¯ÙˆÙ† API Ø®Ø§Ø±Ø¬ÛŒ)
                if not is_fashion_related(title, description):
                    print(f"[SKIP] ØºÛŒØ±Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ù…Ø¯ Ùˆ ÙØ´Ù†: {title[:70]}")
                    continue

                # Ø³Ø§Ø®Øª hash Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ù…Ø­ØªÙˆØ§ÛŒ Ù…Ø´Ø§Ø¨Ù‡
                content_for_hash = (title.lower().strip() + " " + description[:150].lower().strip())
                content_hash = hashlib.sha256(content_for_hash.encode('utf-8')).hexdigest()

                # Ú†Ú© ØªÚ©Ø±Ø§Ø±ÛŒ Ø¨Ø§ Appwrite (Ø¨Ø§ requests Ø®Ø§Ù…)
                is_duplicate = False
                try:
                    # Ú†Ú© Ù„ÛŒÙ†Ú©
                    params_link = {'queries[0]': f'equal("link", ["{link}"])', 'limit': 1}
                    res_link = requests.get(
                        f"{endpoint}/databases/{database_id}/collections/{collection_id}/documents",
                        headers=headers,
                        params=params_link,
                        timeout=10
                    )
                    if res_link.status_code == 200:
                        data_link = res_link.json()
                        if data_link.get('total', 0) > 0:
                            is_duplicate = True
                            print(f"[SKIP] ØªÚ©Ø±Ø§Ø±ÛŒ (Ù„ÛŒÙ†Ú©): {title[:70]}")
                    else:
                        print(f"[WARN] Ø®Ø·Ø§ Ú†Ú© Ù„ÛŒÙ†Ú©: {res_link.status_code} - {res_link.text}")

                    # Ú†Ú© hash Ø§Ú¯Ø± Ù„ÛŒÙ†Ú© ØªÚ©Ø±Ø§Ø±ÛŒ Ù†Ø¨ÙˆØ¯
                    if not is_duplicate:
                        params_hash = {'queries[0]': f'equal("content_hash", ["{content_hash}"])', 'limit': 1}
                        res_hash = requests.get(
                            f"{endpoint}/databases/{database_id}/collections/{collection_id}/documents",
                            headers=headers,
                            params=params_hash,
                            timeout=10
                        )
                        if res_hash.status_code == 200:
                            data_hash = res_hash.json()
                            if data_hash.get('total', 0) > 0:
                                is_duplicate = True
                                print(f"[SKIP] ØªÚ©Ø±Ø§Ø±ÛŒ (Ù…Ø­ØªÙˆØ§): {title[:70]}")
                        else:
                            print(f"[WARN] Ø®Ø·Ø§ Ú†Ú© hash: {res_hash.status_code} - {res_hash.text}")
                except Exception as e:
                    print(f"[WARN] Ø®Ø·Ø§ Ø¯Ø± Ú†Ú© ØªÚ©Ø±Ø§Ø±ÛŒ: {str(e)} - Ø§Ø¯Ø§Ù…Ù‡ Ø¨Ø¯ÙˆÙ† Ú†Ú©")

                if is_duplicate:
                    continue

                # ÙØ±Ù…Øª Ù¾Ø³Øª Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ù…Ø¯ Ùˆ ÙØ´Ù† (Ø¨Ø¯ÙˆÙ† Ø¬Ù…Ù„Ù‡ ØªÚ©Ø±Ø§Ø±ÛŒ)
                final_text = (
                    f"ğŸ’  <b>{title}</b>\n\n"
                    f"{description}\n\n"
                    f"#Ù…Ø¯ #Ø§Ø³ØªØ§ÛŒÙ„ #ØªØ±Ù†Ø¯ #ÙØ´Ù†_Ø§ÛŒØ±Ø§Ù†ÛŒ #Ù…Ù‡Ø±Ø¬Ø§Ù…Ù‡\n"
                    f"ğŸ†” @irfashionnews"
                )

                # Ø¹Ú©Ø³ Ø§Ø² RSS ÛŒØ§ og:image ØµÙØ­Ù‡
                image_url = None
                if 'enclosure' in entry and entry.enclosure.get('type', '').startswith('image/'):
                    image_url = entry.enclosure.href
                elif 'media_content' in entry:
                    for media in entry.media_content:
                        if media.get('medium') == 'image' and media.get('url'):
                            image_url = media['url']
                            break

                # Ø§Ú¯Ø± RSS Ø¹Ú©Ø³ Ù†Ø¯Ø§Ø´ØªØŒ Ø§Ø² ØµÙØ­Ù‡ Ø®Ø¨Ø± Ø¨Ú©Ø´
                if not image_url:
                    image_url = get_og_image_from_page(link)

                try:
                    if image_url:
                        await bot.send_photo(
                            chat_id=chat_id,
                            photo=image_url,
                            caption=final_text,
                            parse_mode='HTML',
                            disable_notification=True
                        )
                    else:
                        await bot.send_message(
                            chat_id=chat_id,
                            text=final_text,
                            parse_mode='HTML',
                            link_preview_options=LinkPreviewOptions(is_disabled=False),
                            disable_notification=True
                        )

                    posted = True
                    print(f"[SUCCESS] Ø§Ø±Ø³Ø§Ù„ Ù…ÙˆÙÙ‚: {title[:70]}")

                    # Ø°Ø®ÛŒØ±Ù‡ Ù„ÛŒÙ†Ú© Ùˆ hash Ø¨Ø§ requests Ø®Ø§Ù…
                    try:
                        payload = {
                            'documentId': 'unique()',
                            'data': {
                                'link': link,
                                'title': title[:300],
                                'content_hash': content_hash,
                                'created_at': now.isoformat(),
                                'source_type': get_source_type(url)  # ÙÛŒÙ„Ø¯ Ù…Ù†Ø¨Ø¹ ÙØ§Ø±Ø³ÛŒ
                            }
                        }
                        res = requests.post(
                            f"{endpoint}/databases/{database_id}/collections/{collection_id}/documents",
                            headers=headers,
                            json=payload,
                            timeout=10
                        )
                        if res.status_code in (200, 201):
                            print("[DB] Ø°Ø®ÛŒØ±Ù‡ Ù…ÙˆÙÙ‚")
                        else:
                            print(f"[WARN] Ø°Ø®ÛŒØ±Ù‡ Ø´Ú©Ø³Øª: {res.status_code} - {res.text}")
                    except Exception as save_err:
                        print(f"[WARN] Ø®Ø·Ø§ Ø°Ø®ÛŒØ±Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³: {str(save_err)}")

                except Exception as send_err:
                    print(f"[ERROR] Ø®Ø·Ø§ Ø§Ø±Ø³Ø§Ù„: {str(send_err)}")

        except Exception as feed_err:
            print(f"[ERROR] Ù…Ø´Ú©Ù„ Ø¯Ø± ÙÛŒØ¯ {url}: {str(feed_err)}")

    print(f"[INFO] Ù¾Ø§ÛŒØ§Ù† Ø§Ø¬Ø±Ø§ - Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯: {posted}")
    return {"status": "success", "posted": posted}


def is_fashion_related(title, description):
    # ÙÛŒÙ„ØªØ± Ø³Ø§Ø¯Ù‡ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ù…Ø¯ Ùˆ ÙØ´Ù† (Ø¯Ø§Ø®Ù„ Ú©Ø¯ØŒ Ø¨Ø¯ÙˆÙ† API)
    keywords = [
        'Ù…Ø¯', 'ÙØ´Ù†', 'Ø§Ø³ØªØ§ÛŒÙ„', 'Ø²ÛŒØ¨Ø§ÛŒÛŒ', 'Ù„Ø¨Ø§Ø³', 'Ù¾ÙˆØ´Ø§Ú©', 'Ø·Ø±Ø§Ø­ÛŒ Ù„Ø¨Ø§Ø³', 'ØªØ±Ù†Ø¯', 'Ú©ÙØ´', 'Ù…Ø§Ù†ØªÙˆ', 'Ø´Ø§Ù„', 'Ø±ÙˆØ³Ø±ÛŒ',
        'fashion', 'style', 'beauty', 'clothing', 'trend', 'outfit', 'couture', 'runway', 'collection', 'designer'
    ]
    combined = (title + ' ' + description).lower()
    return any(kw in combined for kw in keywords)


def get_source_type(feed_url):
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø§Ù… ÙØ§Ø±Ø³ÛŒ Ù…Ù†Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„Ø¯ source_type
    mapping = {
        "medopia.ir": "Ù…Ø¯ÙˆÙ¾ÛŒØ§",
        "digistyle.com": "Ø¯ÛŒØ¬ÛŒâ€ŒØ§Ø³ØªØ§ÛŒÙ„",
        "chibepoosham.com": "Ú†ÛŒ Ø¨Ù¾ÙˆØ´Ù…",
        "tarahanelebas.com": "Ø·Ø±Ø§Ø­Ø§Ù† Ù„Ø¨Ø§Ø³",
        "persianpood.com": "Ù¾Ø±Ø´ÛŒÙ† Ù¾ÙˆØ¯",
        "jument.style": "Ú˜ÙˆÙ…Ù†Øª",
        "zibamoon.com": "Ø²ÛŒØ¨Ø§Ù…ÙˆÙ†",
        "sarak-co.com": "Ø³Ø§Ø±Ú©",
        "elsana.com": "Ø§Ù„Ø³Ø§Ù†Ø§",
        "beytoote.com": "Ø¨ÛŒØªÙˆØªÙ‡",
        "namnak.com": "Ù†Ø§Ù…Ù†Ú©",
        "modetstyle.com": "Ù…ÙˆØ¯Øª Ø§Ø³ØªØ§ÛŒÙ„",
        "antikstyle.com": "Ø¢Ù†ØªÛŒÚ© Ø§Ø³ØªØ§ÛŒÙ„",
        "rnsfashion.com": "Ø¢Ø± Ø§Ù† Ø§Ø³ ÙØ´Ù†",
        "pattonjameh.com": "Ù¾Ø§ØªÙ† Ø¬Ø§Ù…Ù‡",
        "tonikaco.com": "ØªÙˆÙ†ÛŒÚ©Ø§",
        "zoomit.ir": "Ø²ÙˆÙ…ÛŒØª Ø²ÛŒØ¨Ø§ÛŒÛŒ",
        "khabaronline.ir": "Ø®Ø¨Ø±Ø¢Ù†Ù„Ø§ÛŒÙ† Ù…Ø¯",
        "fararu.com": "ÙØ±Ø§Ø±Ùˆ Ù…Ø¯",
        "digikala.com": "Ø¯ÛŒØ¬ÛŒâ€ŒÚ©Ø§Ù„Ø§ Ù…Ø¯",
    }
    
    for domain, name in mapping.items():
        if domain in feed_url:
            return name
    
    return "Ù…Ù†Ø¨Ø¹ Ù†Ø§Ù…Ø´Ø®Øµ"


def get_og_image_from_page(link):
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        response = requests.get(link, timeout=10, headers=headers)
        if response.status_code != 200:
            return None

        soup = BeautifulSoup(response.text, 'html.parser')
        og_image = soup.find('meta', attrs={'property': 'og:image'})
        if og_image and og_image.get('content'):
            return og_image['content']

        # Ø§Ú¯Ø± og:image Ù†Ø¨ÙˆØ¯ØŒ Ø§ÙˆÙ„ÛŒÙ† img Ø¨Ø²Ø±Ú¯
        for img in soup.find_all('img'):
            src = img.get('src') or img.get('data-src')
            if src and len(src) > 15 and 'logo' not in src.lower() and 'icon' not in src.lower():
                return src
        return None
    except Exception as e:
        print(f"[WARN] Ø®Ø·Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ú©Ø³: {str(e)}")
        return None


if __name__ == "__main__":
    asyncio.run(main())