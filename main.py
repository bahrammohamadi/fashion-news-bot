# ============================================================
# Telegram Fashion News Bot â€” @irfashionnews
# Version:    11.0 â€” Iranian Fashion Brands Focus
# Runtime:    Python 3.12 / Appwrite Cloud Functions
# Timeout:    120 seconds
#
# POST FLOW (guaranteed order):
#   â‘  Fetch RSS feeds from Iranian fashion brand sources
#   â‘¡ Filter by fashion relevance + time (weekly window)
#   â‘¢ Check Appwrite DB â€” strict duplicate check (link + hash)
#   â‘£ Extract 1â€“5 images per post
#   â‘¤ send_media_group(all images, NO caption)
#      â†’ anchor_id = last_sent_message.message_id
#   â‘¥ asyncio.sleep(2.5s)
#   â‘¦ send_message(caption, reply_to=anchor_id)
#      â†’ reply dependency = protocol-level order guarantee
#   â‘§ asyncio.sleep(1.5s)
#   â‘¨ send_sticker(random) [non-fatal]
#   â‘© Save record to Appwrite DB
#
# CAPTION FORMAT (HTML, magazine style):
#   ğŸ·ï¸ Brand Name
#   ğŸ’  Product / Post Title
#   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#   Key details (â‰¤ 350 chars)
#
#   ğŸ”— Ø§Ø¯Ø§Ù…Ù‡ Ù…Ø·Ù„Ø¨ | ğŸ†” @irfashionnews
#
#   #BrandName #Ù…Ø¯ #Ø§Ø³ØªØ§ÛŒÙ„ #ØªØ±Ù†Ø¯ #ÙØ´Ù†_Ø§ÛŒØ±Ø§Ù†ÛŒ #Ø¨Ø±Ù†Ø¯_Ø§ÛŒØ±Ø§Ù†ÛŒ
# ============================================================

import os
import asyncio
import hashlib
import random
import requests
import feedparser

from datetime import datetime, timedelta, timezone
from bs4 import BeautifulSoup
from telegram import Bot, InputMediaPhoto, LinkPreviewOptions
from telegram.error import TelegramError


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SECTION 1 â€” IRANIAN FASHION BRAND RSS FEEDS
#
# Each entry is a dict with:
#   url   â†’ RSS/Atom feed URL
#   brand â†’ Brand display name (Persian + English)
#   tag   â†’ Hashtag string for this brand
#
# How to find a brand's RSS feed:
#   1. Visit brand website
#   2. Try appending /feed, /rss, /feed.xml, /atom.xml
#   3. Check <link rel="alternate" type="application/rss+xml">
#   4. For WooCommerce shops: /?feed=rss2 or /feed/
#   5. For WordPress blogs: /feed/ always works
#
# Feeds marked [NEEDS_VERIFICATION] require manual check â€”
# the URL pattern is standard for their platform but may
# need adjustment if the brand uses a custom setup.
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

BRAND_FEEDS: list[dict] = [

    # â”€â”€ La Femme Roje â”€â”€
    {
        "url":   "https://lafemmeroje.com/feed/",
        "brand": "La Femme Roje | Ù„Ø§ ÙÙ… Ø±ÙˆÚ˜",
        "tag":   "#LaFemmeRoje #Ù„Ø§ÙÙ…_Ø±ÙˆÚ˜",
    },

    # â”€â”€ Salian â”€â”€
    {
        "url":   "https://salian.ir/feed/",
        "brand": "Salian | Ø³Ø§Ù„ÛŒØ§Ù†",
        "tag":   "#Salian #Ø³Ø§Ù„ÛŒØ§Ù†",
    },

    # â”€â”€ Celebon â”€â”€
    {
        "url":   "https://celebon.com/feed/",
        "brand": "Celebon | Ø³Ù„Ø¨ÙˆÙ†",
        "tag":   "#Celebon #Ø³Ù„Ø¨ÙˆÙ†",
    },

    # â”€â”€ Siawood â”€â”€
    {
        "url":   "https://siawood.com/feed/",
        "brand": "Siawood | Ø³ÛŒØ§ÙˆÙˆØ¯",
        "tag":   "#Siawood #Ø³ÛŒØ§ÙˆÙˆØ¯",
    },

    # â”€â”€ Naghmeh Kiumarsi â”€â”€
    {
        "url":   "https://naghmehkiumarsi.com/feed/",
        "brand": "Naghmeh Kiumarsi | Ù†ØºÙ…Ù‡ Ú©ÛŒÙˆÙ…Ø±Ø«ÛŒ",
        "tag":   "#NaghmehKiumarsi #Ù†ØºÙ…Ù‡_Ú©ÛŒÙˆÙ…Ø±Ø«ÛŒ",
    },

    # â”€â”€ Poosh â”€â”€
    {
        "url":   "https://pooshmode.com/feed/",
        "brand": "Poosh | Ù¾ÙˆØ´",
        "tag":   "#Poosh #Ù¾ÙˆØ´_Ù…Ø¯",
    },

    # â”€â”€ Kimia â”€â”€
    {
        "url":   "https://kimiamode.com/feed/",
        "brand": "Kimia | Ú©ÛŒÙ…ÛŒØ§",
        "tag":   "#Kimia #Ú©ÛŒÙ…ÛŒØ§_Ù…Ø¯",
    },

    # â”€â”€ Mihano Momosa â”€â”€
    {
        "url":   "https://mihanomomosa.com/feed/",
        "brand": "Mihano Momosa | Ù…ÛŒÙ‡Ø§Ù†Ùˆ Ù…ÙˆÙ…ÙˆØ³Ø§",
        "tag":   "#MihanoMomosa #Ù…ÛŒÙ‡Ø§Ù†Ùˆ_Ù…ÙˆÙ…ÙˆØ³Ø§",
    },

    # â”€â”€ Taghcheh â”€â”€
    {
        "url":   "https://taghcheh.com/feed/",
        "brand": "Taghcheh | Ø·Ø§Ù‚Ú†Ù‡",
        "tag":   "#Taghcheh #Ø·Ø§Ù‚Ú†Ù‡",
    },

    # â”€â”€ Parmi Manto â”€â”€
    {
        "url":   "https://parmimanto.com/feed/",
        "brand": "Parmi Manto | Ù¾Ø§Ø±Ù…ÛŒ Ù…Ø§Ù†ØªÙˆ",
        "tag":   "#ParmiManto #Ù¾Ø§Ø±Ù…ÛŒ_Ù…Ø§Ù†ØªÙˆ",
    },

    # â”€â”€ Banoo Sara â”€â”€
    {
        "url":   "https://banoosara.com/feed/",
        "brand": "Banoo Sara | Ø¨Ø§Ù†Ùˆ Ø³Ø§Ø±Ø§",
        "tag":   "#BanooSara #Ø¨Ø§Ù†Ùˆ_Ø³Ø§Ø±Ø§",
    },

    # â”€â”€ Roshanak â”€â”€
    {
        "url":   "https://roshanakmode.com/feed/",
        "brand": "Roshanak | Ø±Ø´Ù†Ú©",
        "tag":   "#Roshanak #Ø±Ø´Ù†Ú©",
    },

    # â”€â”€ Bodyspinner â”€â”€
    {
        "url":   "https://bodyspinner.com/feed/",
        "brand": "Bodyspinner | Ø¨Ø§Ø¯ÛŒ Ø§Ø³Ù¾ÛŒÙ†Ø±",
        "tag":   "#Bodyspinner #Ø¨Ø§Ø¯ÛŒ_Ø§Ø³Ù¾ÛŒÙ†Ø±",
    },

    # â”€â”€ Garoudi â”€â”€
    {
        "url":   "https://garoudi.com/feed/",
        "brand": "Garoudi | Ú¯Ø§Ø±ÙˆØ¯ÛŒ",
        "tag":   "#Garoudi #Ú¯Ø§Ø±ÙˆØ¯ÛŒ",
    },

    # â”€â”€ Hacoupian â”€â”€
    {
        "url":   "https://hacoupian.com/feed/",
        "brand": "Hacoupian | Ù‡Ø§Ú©ÙˆÙ¾ÛŒØ§Ù†",
        "tag":   "#Hacoupian #Ù‡Ø§Ú©ÙˆÙ¾ÛŒØ§Ù†",
    },

    # â”€â”€ Holiday â”€â”€
    {
        "url":   "https://holidayfashion.ir/feed/",
        "brand": "Holiday | Ù‡Ø§Ù„ÛŒØ¯ÛŒ",
        "tag":   "#Holiday #Ù‡Ø§Ù„ÛŒØ¯ÛŒ",
    },

    # â”€â”€ LC Man â”€â”€
    {
        "url":   "https://lcman.ir/feed/",
        "brand": "LC Man | Ø§Ù„ Ø³ÛŒ Ù…Ù†",
        "tag":   "#LCMan #Ø§Ù„_Ø³ÛŒ_Ù…Ù†",
    },

    # â”€â”€ Narbon â”€â”€
    {
        "url":   "https://narbon.ir/feed/",
        "brand": "Narbon | Ù†Ø§Ø±Ø¨Ù†",
        "tag":   "#Narbon #Ù†Ø§Ø±Ø¨Ù†",
    },

    # â”€â”€ Narian â”€â”€
    {
        "url":   "https://narian.ir/feed/",
        "brand": "Narian | Ù†Ø§Ø±ÛŒØ§Ù†",
        "tag":   "#Narian #Ù†Ø§Ø±ÛŒØ§Ù†",
    },

    # â”€â”€ Patan Jameh â”€â”€
    {
        "url":   "https://patanjameh.com/feed/",
        "brand": "Patan Jameh | Ù¾Ø§ØªØ§Ù† Ø¬Ø§Ù…Ù‡",
        "tag":   "#PatanJameh #Ù¾Ø§ØªØ§Ù†_Ø¬Ø§Ù…Ù‡",
    },

    # â”€â”€ General Persian fashion aggregators (fallback) â”€â”€
    {
        "url":   "https://medopia.ir/feed/",
        "brand": "Medopia | Ù…Ø¯ÙˆÙ¾ÛŒØ§",
        "tag":   "#Medopia #Ù…Ø¯ÙˆÙ¾ÛŒØ§",
    },
    {
        "url":   "https://www.digistyle.com/mag/feed/",
        "brand": "Digistyle | Ø¯ÛŒØ¬ÛŒâ€ŒØ§Ø³ØªØ§ÛŒÙ„",
        "tag":   "#Digistyle #Ø¯ÛŒØ¬ÛŒ_Ø§Ø³ØªØ§ÛŒÙ„",
    },
    {
        "url":   "https://www.chibepoosham.com/feed/",
        "brand": "Chi Be Poosham | Ú†ÛŒ Ø¨Ù¾ÙˆØ´Ù…",
        "tag":   "#ChibePooosham #Ú†ÛŒ_Ø¨Ù¾ÙˆØ´Ù…",
    },
]

# â”€â”€ Fashion relevance: must match at least ONE â”€â”€
POSITIVE_KEYWORDS = [
    # Persian
    'Ù…Ø¯', 'ÙØ´Ù†', 'Ø§Ø³ØªØ§ÛŒÙ„', 'Ø²ÛŒØ¨Ø§ÛŒÛŒ', 'Ù„Ø¨Ø§Ø³', 'Ù¾ÙˆØ´Ø§Ú©',
    'Ø·Ø±Ø§Ø­ÛŒ Ù„Ø¨Ø§Ø³', 'ØªØ±Ù†Ø¯', 'Ú©Ù„Ú©Ø³ÛŒÙˆÙ†', 'Ø¨Ø±Ù†Ø¯', 'Ø³ÛŒØ²Ù†',
    'Ø¢Ø±Ø§ÛŒØ´', 'Ù…Ø§Ù†ØªÙˆ', 'Ù¾ÛŒØ±Ø§Ù‡Ù†', 'Ú©Øª', 'Ø´Ù„ÙˆØ§Ø±', 'Ú©ÛŒÙ',
    'Ú©ÙØ´', 'Ø§Ú©Ø³Ø³ÙˆØ±ÛŒ', 'Ø¬ÙˆØ§Ù‡Ø±', 'Ø·Ù„Ø§', 'Ø¹Ø·Ø±', 'Ù†Ú¯ÛŒÙ†',
    'Ù¾Ø§Ù„ØªÙˆ', 'Ø³Øª Ù„Ø¨Ø§Ø³', 'Ù…Ø²ÙˆÙ†', 'Ø®ÛŒØ§Ø·ÛŒ', 'Ø¨Ø§ÙØª', 'ØªÙˆÙ†ÛŒÚ©',
    'Ø¨Ù„ÙˆØ²', 'Ø¯Ø§Ù…Ù†', 'Ø´Ù†Ù„', 'Ú©Ø§Ù¾Ø´Ù†', 'Ø¬ÙˆØ±Ø§Ø¨', 'Ø±ÙˆØ³Ø±ÛŒ',
    'Ù¾Ø§Ø±Ú†Ù‡', 'Ø·Ø±Ø­', 'Ø¯ÙˆØ®Øª', 'Ø¨Ø±Ù†Ø¯ Ø§ÛŒØ±Ø§Ù†ÛŒ', 'Ù…Ø­ØµÙˆÙ„ Ø¬Ø¯ÛŒØ¯',
    # English
    'fashion', 'style', 'beauty', 'clothing', 'trend',
    'outfit', 'couture', 'collection', 'lookbook', 'brand',
    'wardrobe', 'luxury', 'designer', 'new arrival', 'product',
    'coat', 'dress', 'blouse', 'skirt', 'jacket', 'accessory',
]

# â”€â”€ Hard reject: ANY match = skip â”€â”€
NEGATIVE_KEYWORDS = [
    'ÙÛŒÙ„Ù…', 'Ø³ÛŒÙ†Ù…Ø§', 'Ø³Ø±ÛŒØ§Ù„', 'Ø¨Ø§Ø²ÛŒÚ¯Ø±', 'Ú©Ø§Ø±Ú¯Ø±Ø¯Ø§Ù†', 'Ø§Ø³Ú©Ø§Ø±',
    'ØµØ¨Ø­Ø§Ù†Ù‡', 'Ø±Ú˜ÛŒÙ… ØºØ°Ø§ÛŒÛŒ', 'Ø·Ø±Ø² ØªÙ‡ÛŒÙ‡', 'Ø¯Ø³ØªÙˆØ± Ù¾Ø®Øª', 'Ø¢Ø´Ù¾Ø²ÛŒ',
    'Ø§Ù¾Ù„', 'Ú¯ÙˆÚ¯Ù„', 'Ø¢ÛŒÙÙˆÙ†', 'Ø³Ø§Ù…Ø³ÙˆÙ†Ú¯', 'ØªÚ©Ù†ÙˆÙ„ÙˆÚ˜ÛŒ', 'Ú¯ÛŒÙ…',
    'ÙÙˆØªØ¨Ø§Ù„', 'ÙˆØ§Ù„ÛŒØ¨Ø§Ù„', 'ÙˆØ±Ø²Ø´', 'ØªÛŒÙ… Ù…Ù„ÛŒ', 'Ù„ÛŒÚ¯',
    'Ø¨ÙˆØ±Ø³', 'Ø§Ø±Ø²', 'Ø¯Ù„Ø§Ø±', 'Ø³Ú©Ù‡', 'Ø¨ÛŒØª Ú©ÙˆÛŒÙ†', 'Ø§Ù‚ØªØµØ§Ø¯',
    'Ø§Ù†ØªØ®Ø§Ø¨Ø§Øª', 'Ø³ÛŒØ§Ø³ÛŒ', 'Ù…Ø¬Ù„Ø³', 'Ø¯ÙˆÙ„Øª', 'ÙˆØ²ÛŒØ±',
    'Ø²Ù„Ø²Ù„Ù‡', 'Ø³ÛŒÙ„', 'Ø¢ØªØ´ Ø³ÙˆØ²ÛŒ', 'ØªØµØ§Ø¯Ù', 'Ø­Ø§Ø¯Ø«Ù‡', 'Ú©Ø´ØªÙ‡',
]

# â”€â”€ Fixed hashtag block (always last line of caption) â”€â”€
FIXED_HASHTAGS = (
    "#Ù…Ø¯ #Ø§Ø³ØªØ§ÛŒÙ„ #ØªØ±Ù†Ø¯ #Ø¨Ø±Ù†Ø¯_Ø§ÛŒØ±Ø§Ù†ÛŒ #ÙØ´Ù†_Ø§ÛŒØ±Ø§Ù†ÛŒ "
    "#fashion #IranianFashion #style"
)

# â”€â”€ Limits â”€â”€
MAX_DESCRIPTION_CHARS = 350
MAX_IMAGES            = 5
CAPTION_MAX           = 1020

# â”€â”€ Timeouts (seconds) â”€â”€
FEED_TIMEOUT        = 10
PAGE_TIMEOUT        = 8
DB_TIMEOUT          = 6

# â”€â”€ Weekly scan window (168 hours = 7 days) â”€â”€
HOURS_THRESHOLD     = 168

# â”€â”€ Posting delays â”€â”€
ALBUM_CAPTION_DELAY = 2.5
STICKER_DELAY       = 1.5

# â”€â”€ Image filtering â”€â”€
IMAGE_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.webp', '.gif')
IMAGE_BLOCKLIST  = [
    'doubleclick', 'googletagmanager', 'googlesyndication',
    'facebook.com/tr', 'analytics', 'pixel', 'beacon',
    'tracking', 'counter', 'stat.', 'stats.',
]

# â”€â”€ Fashion stickers â”€â”€
# Replace with real file_ids. Instructions:
#   1. Send any fashion sticker to your bot
#   2. GET https://api.telegram.org/bot<TOKEN>/getUpdates
#   3. Copy result[0].message.sticker.file_id
FASHION_STICKERS = [
    "CAACAgIAAxkBAAIBmGRx1yRFMVhVqVXLv_dAAXJMOdFNAAIUAAOVgnkAAVGGBbBjxbg4LwQ",
    "CAACAgIAAxkBAAIBmWRx1yRqy9JkN2DmV_Z2sRsKdaTjAAIVAAOVgnkAAc8R3q5p5-AELAQ",
    "CAACAgIAAxkBAAIBmmRx1yS2T2gfLqJQX9oK6LZqp1HIAAIWAAO0yXAAAV0MzCRF3ZRILAQ",
    "CAACAgIAAxkBAAIBm2Rx1ySiJV4dVeTuCTc-RfFDnfQpAAIXAAO0yXAAAA3Vm7IiJdisLAQ",
    "CAACAgIAAxkBAAIBnGRx1yT_jVlWt5xPJ7BO9aQ4JvFaAAIYAAO0yXAAAA0k9GZDQpLcLAQ",
]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SECTION 2 â€” MAIN ENTRY POINT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async def main(event=None, context=None):
    print("[INFO] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print("[INFO] Iranian Fashion Brand Bot v11.0 started")
    print(f"[INFO] {datetime.now(timezone.utc).isoformat()}")
    print(f"[INFO] Scanning {len(BRAND_FEEDS)} brand feeds")
    print(f"[INFO] Weekly window: {HOURS_THRESHOLD}h")
    print("[INFO] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

    config = _load_config()
    if not config:
        return {"status": "error", "reason": "missing_env_vars"}

    bot = Bot(token=config["token"])
    db  = _AppwriteDB(
        endpoint      = config["endpoint"],
        project       = config["project"],
        key           = config["key"],
        database_id   = config["database_id"],
        collection_id = config["collection_id"],
    )

    now            = datetime.now(timezone.utc)
    time_threshold = now - timedelta(hours=HOURS_THRESHOLD)

    stats = {
        "feeds_ok":  0,
        "checked":   0,
        "skip_time": 0,
        "skip_filt": 0,
        "skip_dupe": 0,
        "posted":    False,
    }

    for feed_info in BRAND_FEEDS:
        if stats["posted"]:
            break

        feed_url    = feed_info["url"]
        brand_name  = feed_info["brand"]
        brand_tag   = feed_info["tag"]

        print(f"\n[FEED] {brand_name}")
        print(f"       {feed_url}")

        entries = _fetch_feed(feed_url)
        if not entries:
            continue

        stats["feeds_ok"] += 1

        for entry in entries:
            if stats["posted"]:
                break

            stats["checked"] += 1

            # â”€â”€ Parse basic fields â”€â”€
            title = _clean(entry.get("title", ""))
            link  = _clean(entry.get("link",  ""))
            if not title or not link:
                continue

            # â”€â”€ Time filter (weekly window) â”€â”€
            pub_date = _parse_date(entry)
            if pub_date and pub_date < time_threshold:
                stats["skip_time"] += 1
                continue

            # â”€â”€ Description â”€â”€
            raw_html = (
                entry.get("summary")
                or entry.get("description")
                or ""
            )
            desc = _truncate(
                _strip_html(raw_html),
                MAX_DESCRIPTION_CHARS,
            )

            # â”€â”€ Fashion relevance filter â”€â”€
            # For brand-dedicated feeds we are lenient:
            # brand name in title or feed URL counts as positive signal.
            if not _is_fashion(title, desc, feed_url, brand_name):
                stats["skip_filt"] += 1
                print(f"  [SKIP:filter] {title[:60]}")
                continue

            # â”€â”€ Strict duplicate check â”€â”€
            content_hash = _make_hash(title, desc)
            if db.is_duplicate(link, content_hash):
                stats["skip_dupe"] += 1
                print(f"  [SKIP:dupe]   {title[:60]}")
                continue

            print(f"  [CANDIDATE] {title[:60]}")

            # â”€â”€ Collect images â”€â”€
            image_urls = _collect_images(entry, link)

            # â”€â”€ Build caption â”€â”€
            caption = _build_caption(
                title      = title,
                desc       = desc,
                link       = link,
                brand_name = brand_name,
                brand_tag  = brand_tag,
            )

            # â”€â”€ Post to Telegram â”€â”€
            success = await _post_to_telegram(
                bot        = bot,
                chat_id    = config["chat_id"],
                image_urls = image_urls,
                caption    = caption,
            )

            if success:
                stats["posted"] = True
                print(f"  [SUCCESS] Posted: {title[:60]}")
                db.save(
                    link         = link,
                    title        = title,
                    content_hash = content_hash,
                    brand        = brand_name,
                    created_at   = now.isoformat(),
                )

    # â”€â”€ Summary â”€â”€
    print("\n[INFO] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ SUMMARY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print(f"[INFO] Feeds alive : {stats['feeds_ok']} / {len(BRAND_FEEDS)}")
    print(f"[INFO] Checked     : {stats['checked']}")
    print(f"[INFO] Skip/time   : {stats['skip_time']}")
    print(f"[INFO] Skip/filter : {stats['skip_filt']}")
    print(f"[INFO] Skip/dupe   : {stats['skip_dupe']}")
    print(f"[INFO] Posted      : {stats['posted']}")
    print("[INFO] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    return {"status": "success", "posted": stats["posted"]}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SECTION 3 â€” CONFIG LOADER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _load_config() -> dict | None:
    cfg = {
        "token":         os.environ.get("TELEGRAM_BOT_TOKEN"),
        "chat_id":       os.environ.get("TELEGRAM_CHANNEL_ID"),
        "endpoint":      os.environ.get("APPWRITE_ENDPOINT",
                                        "https://cloud.appwrite.io/v1"),
        "project":       os.environ.get("APPWRITE_PROJECT_ID"),
        "key":           os.environ.get("APPWRITE_API_KEY"),
        "database_id":   os.environ.get("APPWRITE_DATABASE_ID"),
        "collection_id": os.environ.get("APPWRITE_COLLECTION_ID", "history"),
    }
    missing = [k for k, v in cfg.items() if not v]
    if missing:
        print(f"[ERROR] Missing env vars: {missing}")
        return None
    return cfg


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SECTION 4 â€” APPWRITE DATABASE CLIENT
#
# Raw requests â€” no SDK dependency.
# Same database + collection as the international bot.
# Checks: link (exact URL) + content_hash (SHA256).
# Saves: link, title, content_hash, brand, created_at.
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class _AppwriteDB:

    def __init__(self, endpoint, project, key, database_id, collection_id):
        self._url = (
            f"{endpoint}/databases/{database_id}"
            f"/collections/{collection_id}/documents"
        )
        self._headers = {
            "Content-Type":       "application/json",
            "X-Appwrite-Project": project,
            "X-Appwrite-Key":     key,
        }

    def is_duplicate(self, link: str, content_hash: str) -> bool:
        """
        Strict check â€” True if EITHER link OR hash already in DB.
        On DB error returns False (do not block posting).
        """
        return (
            self._exists("link",         link[:500])
            or self._exists("content_hash", content_hash)
        )

    def save(self, link: str, title: str, content_hash: str,
             brand: str, created_at: str) -> bool:
        """Persist a new post record after successful delivery."""
        doc_id = hashlib.md5(link.encode()).hexdigest()[:20]
        try:
            resp = requests.post(
                self._url,
                headers=self._headers,
                json={
                    "documentId": doc_id,
                    "data": {
                        "link":         link[:500],
                        "title":        title[:300],
                        "content_hash": content_hash,
                        "brand":        brand[:100],
                        "created_at":   created_at,
                    },
                },
                timeout=DB_TIMEOUT,
            )
            ok = resp.status_code in (200, 201)
            print(
                "[DB] Saved." if ok
                else f"[WARN] DB save {resp.status_code}: {resp.text[:100]}"
            )
            return ok
        except requests.RequestException as e:
            print(f"[WARN] DB save error: {e}")
            return False

    def _exists(self, field: str, value: str) -> bool:
        try:
            resp = requests.get(
                self._url,
                headers=self._headers,
                params={
                    "queries[]": f'equal("{field}", ["{value}"])',
                    "limit":     1,
                },
                timeout=DB_TIMEOUT,
            )
            if resp.status_code == 200:
                found = resp.json().get("total", 0) > 0
                if found:
                    print(f"  [DB] Duplicate by {field}.")
                return found
            print(f"[WARN] DB query {resp.status_code} ({field})")
            return False
        except requests.RequestException as e:
            print(f"[WARN] DB query error ({field}): {e}")
            return False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SECTION 5 â€” RSS FEED FETCHER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _fetch_feed(url: str) -> list:
    """
    Fetch RSS via requests (timeout-safe), parse with feedparser.
    Returns list of entries, or [] on any failure.
    """
    try:
        resp = requests.get(
            url,
            timeout=FEED_TIMEOUT,
            headers={
                "User-Agent": "Mozilla/5.0 (compatible; IranFashionBot/1.0)",
                "Accept":     "application/rss+xml, application/xml, */*",
            },
        )
        if resp.status_code != 200:
            print(f"  [WARN] HTTP {resp.status_code}")
            return []

        feed = feedparser.parse(resp.content)
        if feed.bozo and not feed.entries:
            print(f"  [WARN] Malformed feed")
            return []

        print(f"  [INFO] {len(feed.entries)} entries found.")
        return feed.entries

    except requests.RequestException as e:
        print(f"  [ERROR] Feed fetch: {e}")
        return []
    except Exception as e:
        print(f"  [ERROR] Feed parse: {e}")
        return []


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SECTION 6 â€” TEXT UTILITIES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _clean(text: str) -> str:
    return (text or "").strip()


def _strip_html(html: str) -> str:
    if not html:
        return ""
    soup = BeautifulSoup(html, "lxml")
    for tag in soup(["script", "style", "iframe"]):
        tag.decompose()
    return " ".join(soup.get_text(separator=" ").split())


def _truncate(text: str, limit: int) -> str:
    if len(text) <= limit:
        return text
    cut        = text[:limit]
    last_space = cut.rfind(" ")
    if last_space > limit * 0.8:
        cut = cut[:last_space]
    return cut + "â€¦"


def _make_hash(title: str, desc: str) -> str:
    """SHA256 of normalized title + first 150 chars of description."""
    raw = f"{title.lower().strip()} {desc[:150].lower().strip()}"
    return hashlib.sha256(raw.encode("utf-8")).hexdigest()


def _parse_date(entry) -> datetime | None:
    for field in ("published_parsed", "updated_parsed"):
        parsed = entry.get(field)
        if parsed:
            try:
                return datetime(*parsed[:6], tzinfo=timezone.utc)
            except (ValueError, TypeError):
                continue
    return None


def _escape_html(text: str) -> str:
    return (
        text.replace("&", "&amp;")
            .replace("<", "&lt;")
            .replace(">", "&gt;")
    )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SECTION 7 â€” FASHION RELEVANCE FILTER
#
# For brand-dedicated feeds we apply extra leniency:
# if the brand name or feed domain appears in the text,
# that counts as a positive signal even without explicit
# fashion keywords (product names rarely contain them).
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _is_fashion(
    title: str,
    desc: str,
    feed_url: str,
    brand_name: str,
) -> bool:
    """
    Stage 1: Hard reject if ANY negative keyword found.
    Stage 2: Accept if:
      a) At least one POSITIVE keyword found, OR
      b) Brand name appears in title/desc (brand feed leniency), OR
      c) Feed URL domain matches a known brand domain.
    """
    combined = (title + " " + desc).lower()

    # Stage 1 â€” hard reject
    for kw in NEGATIVE_KEYWORDS:
        if kw in combined:
            return False

    # Stage 2a â€” positive keyword
    for kw in POSITIVE_KEYWORDS:
        if kw in combined:
            return True

    # Stage 2b â€” brand name signal (leniency for brand feeds)
    brand_lower = brand_name.lower()
    brand_parts = [
        p.strip()
        for p in brand_lower.replace("|", " ").split()
        if len(p.strip()) >= 4
    ]
    for part in brand_parts:
        if part in combined:
            return True

    # Stage 2c â€” domain signal
    try:
        from urllib.parse import urlparse
        domain = urlparse(feed_url).netloc.replace("www.", "").lower()
        if domain and domain.split(".")[0] in combined:
            return True
    except Exception:
        pass

    return False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SECTION 8 â€” IMAGE COLLECTION
#
# Collects up to MAX_IMAGES valid image URLs per entry.
# Priority:
#   1. RSS <enclosure type="image/*">
#   2. RSS <media:content>
#   3. RSS <media:thumbnail>
#   4. <img> tags inside RSS description HTML
#   5. og:image / twitter:image from article page (fallback)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _collect_images(entry, article_url: str) -> list[str]:
    images: list[str] = []
    seen:   set[str]  = set()

    def _add(url: str) -> None:
        url = (url or "").strip()
        if not url or not url.startswith("http") or url in seen:
            return
        lower = url.lower()
        if any(b in lower for b in IMAGE_BLOCKLIST):
            return
        base     = lower.split("?")[0]
        has_ext  = any(base.endswith(e) for e in IMAGE_EXTENSIONS)
        has_word = any(
            w in lower
            for w in ["image", "photo", "img", "picture", "media", "cdn",
                      "upload", "product", "wp-content"]
        )
        if not has_ext and not has_word:
            return
        seen.add(url)
        images.append(url)

    # 1. Enclosures
    enclosures = entry.get("enclosures", [])
    if not enclosures and hasattr(entry, "enclosure") and entry.enclosure:
        enclosures = [entry.enclosure]
    for enc in enclosures:
        if isinstance(enc, dict):
            mime = enc.get("type", "")
            href = enc.get("href") or enc.get("url", "")
        else:
            mime = getattr(enc, "type", "")
            href = getattr(enc, "href", "") or getattr(enc, "url", "")
        if mime.startswith("image/") and href:
            _add(href)

    # 2. media:content
    for m in entry.get("media_content", []):
        url    = m.get("url", "")    if isinstance(m, dict) else getattr(m, "url", "")
        medium = m.get("medium", "") if isinstance(m, dict) else getattr(m, "medium", "")
        if medium == "image" or any(url.lower().endswith(e) for e in IMAGE_EXTENSIONS):
            _add(url)

    # 3. media:thumbnail
    for t in entry.get("media_thumbnail", []):
        url = t.get("url", "") if isinstance(t, dict) else getattr(t, "url", "")
        _add(url)

    # 4. <img> in description HTML
    if len(images) < MAX_IMAGES:
        raw_html = (
            entry.get("summary")
            or entry.get("description")
            or (entry.get("content") or [{}])[0].get("value", "")
        )
        if raw_html:
            soup = BeautifulSoup(raw_html, "lxml")
            for img_tag in soup.find_all("img"):
                for attr in ("src", "data-src", "data-lazy-src", "data-original"):
                    src = img_tag.get(attr, "")
                    if src and src.startswith("http"):
                        _add(src)
                        break
                if len(images) >= MAX_IMAGES:
                    break

    # 5. og:image page fallback
    if not images:
        og = _fetch_og_image(article_url)
        if og:
            _add(og)

    result = images[:MAX_IMAGES]
    print(f"  [INFO] Images: {len(result)}")
    return result


def _fetch_og_image(url: str) -> str | None:
    """Fetch article page and extract og:image or twitter:image."""
    try:
        resp = requests.get(
            url,
            timeout=PAGE_TIMEOUT,
            headers={"User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36"
            )},
            allow_redirects=True,
        )
        if resp.status_code != 200:
            return None
        soup = BeautifulSoup(resp.text, "lxml")
        for prop in ("og:image", "twitter:image"):
            tag = (
                soup.find("meta", property=prop)
                or soup.find("meta", attrs={"name": prop})
            )
            if tag:
                content = tag.get("content", "").strip()
                if content.startswith("http"):
                    return content
    except Exception:
        pass
    return None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SECTION 9 â€” CAPTION BUILDER
#
# Structure (top â†’ bottom):
#
#   ğŸ·ï¸ Brand Name
#   ğŸ’  <b>Product Title</b>
#   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#   Description (â‰¤ 350 chars)
#
#   ğŸ”— <a href="link">Ø§Ø¯Ø§Ù…Ù‡ Ù…Ø·Ù„Ø¨</a> | ğŸ†” @irfashionnews
#
#   #BrandTag #Ù…Ø¯ #Ø§Ø³ØªØ§ÛŒÙ„ #ØªØ±Ù†Ø¯ #Ø¨Ø±Ù†Ø¯_Ø§ÛŒØ±Ø§Ù†ÛŒ #ÙØ´Ù†_Ø§ÛŒØ±Ø§Ù†ÛŒ
#   â† always last line
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _build_caption(
    title:      str,
    desc:       str,
    link:       str,
    brand_name: str,
    brand_tag:  str,
) -> str:
    safe_brand = _escape_html(brand_name.strip())
    safe_title = _escape_html(title.strip())
    safe_desc  = _escape_html(desc.strip())

    # Brand-specific hashtags come first, then fixed block
    hashtag_line = f"{brand_tag} {FIXED_HASHTAGS}"

    parts = [
        f"ğŸ·ï¸ {safe_brand}",
        f"ğŸ’  <b>{safe_title}</b>",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€",
        safe_desc,
        f'ğŸ”— <a href="{link}">Ø§Ø¯Ø§Ù…Ù‡ Ù…Ø·Ù„Ø¨</a> | ğŸ†” @irfashionnews',
        hashtag_line,    # always last
    ]

    caption = "\n\n".join(parts)

    # Trim description if over Telegram limit
    if len(caption) > CAPTION_MAX:
        overflow  = len(caption) - CAPTION_MAX
        safe_desc = safe_desc[:max(0, len(safe_desc) - overflow - 5)] + "â€¦"
        parts[3]  = safe_desc
        caption   = "\n\n".join(parts)

    return caption


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SECTION 10 â€” TELEGRAM POSTING
#
# ORDER GUARANTEE via reply_to_message_id:
#
#   â‘  send_media_group(all images, NO caption)
#      â†’ anchor_id = last_sent_message.message_id
#   â‘¡ asyncio.sleep(ALBUM_CAPTION_DELAY = 2.5s)
#   â‘¢ send_message(caption, reply_to=anchor_id)
#      â†’ A Telegram reply cannot be delivered before its parent.
#        Order is enforced at protocol level, not by timing alone.
#   â‘£ asyncio.sleep(STICKER_DELAY = 1.5s)
#   â‘¤ send_sticker(random) [non-fatal]
#
# EDGE CASES:
#   â‰¥2 images â†’ send_media_group â†’ anchor â†’ reply caption
#    1 image  â†’ send_photo (no caption) â†’ anchor â†’ reply caption
#    0 images â†’ skip image step â†’ standalone caption
#
# FALLBACK CHAIN:
#   send_media_group fails â†’ try send_photo(images[0])
#   send_photo fails       â†’ proceed without anchor
#   send_message fails     â†’ return False (post not counted)
#   send_sticker fails     â†’ log warn, return True anyway
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async def _post_to_telegram(
    bot:        Bot,
    chat_id:    str,
    image_urls: list[str],
    caption:    str,
) -> bool:
    """
    Full post sequence.
    Returns True only if the caption message was delivered.
    """
    anchor_msg_id: int | None = None

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # STEP â‘   Send images (no caption)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if len(image_urls) >= 2:
        try:
            media_group = [
                InputMediaPhoto(media=url)
                for url in image_urls[:MAX_IMAGES]
            ]
            sent_list = await bot.send_media_group(
                chat_id=chat_id,
                media=media_group,
                disable_notification=True,
            )
            anchor_msg_id = sent_list[-1].message_id
            print(
                f"  [INFO] â‘  Album: {len(sent_list)} images. "
                f"anchor={anchor_msg_id}"
            )
        except TelegramError as e:
            print(f"  [WARN] â‘  Album failed: {e}")
            if image_urls:
                try:
                    sent = await bot.send_photo(
                        chat_id=chat_id,
                        photo=image_urls[0],
                        disable_notification=True,
                    )
                    anchor_msg_id = sent.message_id
                    print(
                        f"  [INFO] â‘  Fallback photo. "
                        f"anchor={anchor_msg_id}"
                    )
                except TelegramError as e2:
                    print(f"  [WARN] â‘  Fallback photo failed: {e2}")

    elif len(image_urls) == 1:
        try:
            sent = await bot.send_photo(
                chat_id=chat_id,
                photo=image_urls[0],
                disable_notification=True,
            )
            anchor_msg_id = sent.message_id
            print(f"  [INFO] â‘  Single photo. anchor={anchor_msg_id}")
        except TelegramError as e:
            print(f"  [WARN] â‘  Single photo failed: {e}")

    else:
        print("  [INFO] â‘  No images â€” caption will be standalone.")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # STEP â‘¡  Hard delay
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if anchor_msg_id is not None:
        print(f"  [INFO] â‘¡ Waiting {ALBUM_CAPTION_DELAY}sâ€¦")
        await asyncio.sleep(ALBUM_CAPTION_DELAY)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # STEP â‘¢  Send caption (reply to anchor)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    try:
        kwargs: dict = {
            "chat_id":              chat_id,
            "text":                 caption,
            "parse_mode":           "HTML",
            "link_preview_options": LinkPreviewOptions(is_disabled=True),
            "disable_notification": True,
        }
        if anchor_msg_id is not None:
            kwargs["reply_to_message_id"] = anchor_msg_id

        await bot.send_message(**kwargs)

        label = (
            f"reply_to={anchor_msg_id}"
            if anchor_msg_id is not None else "standalone"
        )
        print(f"  [INFO] â‘¢ Caption sent ({label}).")

    except TelegramError as e:
        print(f"  [ERROR] â‘¢ Caption failed: {e}")
        return False

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # STEPS â‘£â‘¤  Sticker (non-fatal)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if FASHION_STICKERS:
        await asyncio.sleep(STICKER_DELAY)
        try:
            await bot.send_sticker(
                chat_id=chat_id,
                sticker=random.choice(FASHION_STICKERS),
                disable_notification=True,
            )
            print("  [INFO] â‘¤ Sticker sent.")
        except TelegramError as e:
            print(f"  [WARN] â‘¤ Sticker failed (non-fatal): {e}")

    return True


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ENTRY POINT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    asyncio.run(main())
