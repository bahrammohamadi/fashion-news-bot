import os
import asyncio
import feedparser
import requests
import hashlib
import json
from datetime import datetime, timedelta, timezone
from telegram import Bot, LinkPreviewOptions
from telegram.error import TelegramError
from bs4 import BeautifulSoup


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  CONFIGURATION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

RSS_FEEDS = [
    # âœ… Dedicated fashion/style sources (highest priority)
    "https://medopia.ir/feed/",
    "https://www.digistyle.com/mag/feed/",
    "https://www.chibepoosham.com/feed/",
    "https://www.tarahanelebas.com/feed/",
    "https://www.persianpood.com/feed/",
    "https://www.zibamoon.com/feed/",
    "https://www.elsana.com/feed/",
    "https://www.beytoote.com/rss/fashion",
    "https://www.namnak.com/rss/fashion",
    "https://www.roozaneh.net/rss/fashion",
    "https://www.bartarinha.ir/rss/fashion",
    # âœ… General news - fashion category only
    "https://www.zoomit.ir/feed/category/fashion-beauty/",
    "https://fararu.com/rss/category/Ù…Ø¯-Ø²ÛŒØ¨Ø§ÛŒÛŒ",
    "https://www.digikala.com/mag/feed/?category=Ù…Ø¯-Ùˆ-Ø²ÛŒØ¨Ø§ÛŒÛŒ",
]

# Keywords that MUST appear for the post to be published
POSITIVE_KEYWORDS = [
    'Ù…Ø¯', 'ÙØ´Ù†', 'Ø§Ø³ØªØ§ÛŒÙ„', 'Ø²ÛŒØ¨Ø§ÛŒÛŒ', 'Ù„Ø¨Ø§Ø³', 'Ù¾ÙˆØ´Ø§Ú©',
    'Ø·Ø±Ø§Ø­ÛŒ Ù„Ø¨Ø§Ø³', 'ØªØ±Ù†Ø¯', 'Ú©Ù„Ú©Ø³ÛŒÙˆÙ†', 'Ø¨Ø±Ù†Ø¯', 'Ø³ÛŒØ²Ù†',
    'fashion', 'style', 'beauty', 'clothing', 'trend',
    'outfit', 'couture', 'runway', 'lookbook', 'textile',
    'Ø¢Ø±Ø§ÛŒØ´', 'Ù…Ø§Ù†ØªÙˆ', 'Ù¾ÛŒØ±Ø§Ù‡Ù†', 'Ú©Øª', 'Ø´Ù„ÙˆØ§Ø±', 'Ú©ÛŒÙ',
    'Ú©ÙØ´', 'Ø§Ú©Ø³Ø³ÙˆØ±ÛŒ', 'Ø¬ÙˆØ§Ù‡Ø±', 'Ø·Ù„Ø§', 'Ø¹Ø·Ø±', 'Ù†Ú¯ÛŒÙ†',
]

# Keywords that immediately REJECT the post
NEGATIVE_KEYWORDS = [
    'ÙÛŒÙ„Ù…', 'Ø³ÛŒÙ†Ù…Ø§', 'Ø³Ø±ÛŒØ§Ù„', 'Ø¨Ø§Ø²ÛŒ', 'Ú¯ÛŒÙ…', 'ØªØ±ÛŒÙ„Ø±',
    'Ù†Ù‚Ø¯ ÙÛŒÙ„Ù…', 'Ø¨Ø§Ø²ÛŒÚ¯Ø±', 'Ú©Ø§Ø±Ú¯Ø±Ø¯Ø§Ù†', 'Ø§Ø³Ú©Ø§Ø±',
    'ØµØ¨Ø­Ø§Ù†Ù‡', 'Ø±Ú˜ÛŒÙ… ØºØ°Ø§ÛŒÛŒ', 'Ø·Ø±Ø² ØªÙ‡ÛŒÙ‡', 'Ø¯Ø³ØªÙˆØ± Ù¾Ø®Øª',
    'Ø§Ù¾Ù„', 'Ú¯ÙˆÚ¯Ù„', 'Ù¾ÛŒÚ©Ø³Ù„', 'Ø¢ÛŒÙÙˆÙ†', 'Ø³Ø§Ù…Ø³ÙˆÙ†Ú¯',
    'ÙÙˆØªØ¨Ø§Ù„', 'ÙˆØ§Ù„ÛŒØ¨Ø§Ù„', 'ÙˆØ±Ø²Ø´', 'ØªÛŒÙ… Ù…Ù„ÛŒ', 'Ù„ÛŒÚ¯',
    'Ø¨ÙˆØ±Ø³', 'Ø§Ø±Ø²', 'Ø¯Ù„Ø§Ø±', 'Ø³Ú©Ù‡', 'Ø¨ÛŒØª Ú©ÙˆÛŒÙ†',
    'Ø§Ù†ØªØ®Ø§Ø¨Ø§Øª', 'Ø³ÛŒØ§Ø³ÛŒ', 'Ù…Ø¬Ù„Ø³', 'Ø¯ÙˆÙ„Øª', 'ÙˆØ²ÛŒØ±',
    'Ø²Ù„Ø²Ù„Ù‡', 'Ø³ÛŒÙ„', 'Ø¢ØªØ´ Ø³ÙˆØ²ÛŒ', 'ØªØµØ§Ø¯Ù', 'Ø­Ø§Ø¯Ø«Ù‡',
]

MAX_DESCRIPTION_LENGTH = 350
FEED_TIMEOUT = 10       # seconds for RSS fetch
PAGE_TIMEOUT = 8        # seconds for og:image page fetch
DB_TIMEOUT = 6          # seconds for Appwrite API calls
HOURS_THRESHOLD = 48    # how old can a post be (hours)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  MAIN ENTRY POINT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def main(event=None, context=None):
    print("[INFO] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print("[INFO] Fashion News Bot started")
    print(f"[INFO] Time: {datetime.now(timezone.utc).isoformat()}")
    print("[INFO] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

    # Load environment variables
    config = load_config()
    if not config:
        return {"status": "error", "reason": "missing_env_vars"}

    bot = Bot(token=config['token'])
    appwrite = AppwriteClient(
        endpoint=config['endpoint'],
        project=config['project'],
        key=config['key'],
        database_id=config['database_id'],
        collection_id=config['collection_id'],
    )

    now = datetime.now(timezone.utc)
    time_threshold = now - timedelta(hours=HOURS_THRESHOLD)
    posted = False
    total_checked = 0
    total_skipped_time = 0
    total_skipped_filter = 0
    total_skipped_duplicate = 0

    for feed_url in RSS_FEEDS:
        if posted:
            break

        print(f"\n[FEED] Checking: {feed_url}")
        entries = fetch_feed_entries(feed_url)

        if not entries:
            continue

        for entry in entries:
            if posted:
                break

            total_checked += 1

            # â”€â”€ 1. Parse basic fields â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            title = clean_text(entry.get('title', ''))
            link = clean_text(entry.get('link', ''))

            if not title or not link:
                continue

            # â”€â”€ 2. Time filter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            pub_date = parse_entry_date(entry)
            if pub_date and pub_date < time_threshold:
                total_skipped_time += 1
                continue

            # â”€â”€ 3. Clean description â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            raw = entry.get('summary') or entry.get('description') or ''
            description = strip_html(raw)
            description = truncate(description, MAX_DESCRIPTION_LENGTH)

            # â”€â”€ 4. Fashion relevance filter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if not is_fashion_related(title, description):
                total_skipped_filter += 1
                print(f"[SKIP:filter] {title[:70]}")
                continue

            # â”€â”€ 5. Duplicate check â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            content_hash = make_hash(title, description)

            if appwrite.is_duplicate(link, content_hash):
                total_skipped_duplicate += 1
                print(f"[SKIP:duplicate] {title[:70]}")
                continue

            # â”€â”€ 6. Get image â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            image_url = get_image_from_rss(entry) or get_og_image(link)

            # â”€â”€ 7. Build message â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            message = build_message(title, description, link)

            # â”€â”€ 8. Send to Telegram â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            success = await send_to_telegram(
                bot=bot,
                chat_id=config['chat_id'],
                text=message,
                image_url=image_url
            )

            if success:
                posted = True
                print(f"[SUCCESS] Post sent: {title[:70]}")

                # â”€â”€ 9. Save to database â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                appwrite.save_record(
                    link=link,
                    title=title,
                    content_hash=content_hash,
                    created_at=now.isoformat()
                )

    # â”€â”€ Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n[INFO] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• SUMMARY â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print(f"[INFO] Total checked  : {total_checked}")
    print(f"[INFO] Skipped (time) : {total_skipped_time}")
    print(f"[INFO] Skipped (filter): {total_skipped_filter}")
    print(f"[INFO] Skipped (dupe) : {total_skipped_duplicate}")
    print(f"[INFO] Post sent      : {posted}")
    print("[INFO] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

    return {"status": "success", "posted": posted}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  CONFIGURATION LOADER
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_config():
    """Load and validate all required environment variables."""
    required = {
        'token':         os.environ.get('TELEGRAM_BOT_TOKEN'),
        'chat_id':       os.environ.get('TELEGRAM_CHANNEL_ID'),
        'endpoint':      os.environ.get('APPWRITE_ENDPOINT', 'https://cloud.appwrite.io/v1'),
        'project':       os.environ.get('APPWRITE_PROJECT_ID'),
        'key':           os.environ.get('APPWRITE_API_KEY'),
        'database_id':   os.environ.get('APPWRITE_DATABASE_ID'),
        'collection_id': os.environ.get('APPWRITE_COLLECTION_ID', 'history'),
    }

    missing = [k for k, v in required.items() if not v]
    if missing:
        print(f"[ERROR] Missing environment variables: {missing}")
        return None

    return required


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  APPWRITE CLIENT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class AppwriteClient:
    """Handles all Appwrite database operations."""

    def __init__(self, endpoint, project, key, database_id, collection_id):
        self.base_url = f"{endpoint}/databases/{database_id}/collections/{collection_id}/documents"
        self.headers = {
            'Content-Type': 'application/json',
            'X-Appwrite-Project': project,
            'X-Appwrite-Key': key,
        }

    def is_duplicate(self, link: str, content_hash: str) -> bool:
        """
        Check if post already exists by link OR content hash.
        Uses correct Appwrite v1 query format.
        """
        # â”€â”€ Check by link â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if self._query_exists('link', link):
            return True

        # â”€â”€ Check by content hash â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if self._query_exists('content_hash', content_hash):
            return True

        return False

    def _query_exists(self, field: str, value: str) -> bool:
        """Run an Appwrite equal() query and return True if any document found."""
        try:
            # âœ… Correct Appwrite REST query format
            params = {
                'queries[]': f'equal("{field}", ["{value}"])',
                'limit': 1,
            }
            res = requests.get(
                self.base_url,
                headers=self.headers,
                params=params,
                timeout=DB_TIMEOUT
            )

            if res.status_code == 200:
                return res.json().get('total', 0) > 0

            print(f"[WARN] Appwrite query returned {res.status_code}: {res.text[:100]}")
            return False

        except requests.RequestException as e:
            print(f"[WARN] Appwrite query error: {e}")
            return False  # On network error, don't block posting

    def save_record(self, link: str, title: str, content_hash: str, created_at: str) -> bool:
        """
        Save a new document to Appwrite.
        âœ… Uses correct Appwrite REST API structure.
        """
        try:
            # Generate a unique document ID
            doc_id = hashlib.md5(link.encode()).hexdigest()[:20]

            # âœ… Correct Appwrite document creation payload
            payload = {
                'documentId': doc_id,
                'data': {
                    'link': link[:500],
                    'title': title[:300],
                    'content_hash': content_hash,
                    'created_at': created_at,
                }
            }

            res = requests.post(
                self.base_url,
                headers=self.headers,
                json=payload,
                timeout=DB_TIMEOUT
            )

            if res.status_code in (200, 201):
                print("[DB] Record saved successfully")
                return True
            else:
                print(f"[WARN] DB save failed ({res.status_code}): {res.text[:150]}")
                return False

        except requests.RequestException as e:
            print(f"[WARN] DB save error: {e}")
            return False


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  RSS FEED FETCHER
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def fetch_feed_entries(url: str) -> list:
    """
    Fetch and parse RSS feed entries.
    Returns empty list on any error.
    """
    try:
        # feedparser can hang on slow feeds - use requests with timeout first
        response = requests.get(
            url,
            timeout=FEED_TIMEOUT,
            headers={
                'User-Agent': 'Mozilla/5.0 (compatible; FashionBot/1.0)',
                'Accept': 'application/rss+xml, application/xml, text/xml, */*',
            }
        )

        if response.status_code != 200:
            print(f"[WARN] Feed returned {response.status_code}: {url}")
            return []

        feed = feedparser.parse(response.content)

        if feed.bozo and not feed.entries:
            print(f"[WARN] Malformed feed: {url}")
            return []

        entries = feed.entries
        print(f"[INFO] Found {len(entries)} entries in feed")
        return entries

    except requests.RequestException as e:
        print(f"[ERROR] Feed fetch failed ({url}): {e}")
        return []
    except Exception as e:
        print(f"[ERROR] Feed parse error ({url}): {e}")
        return []


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  TEXT UTILITIES
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def clean_text(text: str) -> str:
    """Strip whitespace and normalize."""
    return (text or '').strip()


def strip_html(html: str) -> str:
    """Remove HTML tags and return clean text."""
    if not html:
        return ''
    soup = BeautifulSoup(html, 'lxml')
    # Remove script and style elements
    for tag in soup(['script', 'style', 'iframe']):
        tag.decompose()
    return ' '.join(soup.get_text(separator=' ').split())


def truncate(text: str, max_length: int) -> str:
    """Truncate text at word boundary."""
    if len(text) <= max_length:
        return text
    truncated = text[:max_length]
    last_space = truncated.rfind(' ')
    if last_space > max_length * 0.8:
        truncated = truncated[:last_space]
    return truncated + '...'


def make_hash(title: str, description: str) -> str:
    """Create SHA256 hash for duplicate detection."""
    content = f"{title.lower().strip()} {description[:150].lower().strip()}"
    return hashlib.sha256(content.encode('utf-8')).hexdigest()


def parse_entry_date(entry) -> datetime | None:
    """Safely parse RSS entry date to UTC datetime."""
    for field in ('published_parsed', 'updated_parsed'):
        parsed = entry.get(field)
        if parsed:
            try:
                return datetime(*parsed[:6], tzinfo=timezone.utc)
            except (ValueError, TypeError):
                continue
    return None  # No date = don't skip (be inclusive)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  FASHION RELEVANCE FILTER
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def is_fashion_related(title: str, description: str) -> bool:
    """
    Two-stage filter:
    1. Reject if any negative keyword found
    2. Accept only if at least one positive keyword found
    """
    combined = (title + ' ' + description).lower()

    # Stage 1: Hard rejection
    for kw in NEGATIVE_KEYWORDS:
        if kw in combined:
            return False

    # Stage 2: Must have fashion content
    for kw in POSITIVE_KEYWORDS:
        if kw in combined:
            return True

    return False


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  IMAGE EXTRACTION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_image_from_rss(entry) -> str | None:
    """
    Extract image URL from RSS entry.
    Handles multiple RSS image formats.
    """
    # â”€â”€ Method 1: <enclosure> tag â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    enclosures = entry.get('enclosures', [])
    # feedparser sometimes puts single enclosure as entry.enclosure
    if not enclosures and hasattr(entry, 'enclosure'):
        enclosures = [entry.enclosure]

    for enc in enclosures:
        if isinstance(enc, dict):
            mime = enc.get('type', '')
            url = enc.get('href') or enc.get('url', '')
        else:
            mime = getattr(enc, 'type', '')
            url = getattr(enc, 'href', '') or getattr(enc, 'url', '')

        if mime.startswith('image/') and url:
            return url

    # â”€â”€ Method 2: <media:content> tag â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    media_content = entry.get('media_content', [])
    for media in media_content:
        if isinstance(media, dict):
            url = media.get('url', '')
            medium = media.get('medium', '')
        else:
            url = getattr(media, 'url', '')
            medium = getattr(media, 'medium', '')

        if url and (medium == 'image' or url.lower().endswith(('.jpg', '.jpeg', '.png', '.webp'))):
            return url

    # â”€â”€ Method 3: <media:thumbnail> tag â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    media_thumbnail = entry.get('media_thumbnail', [])
    for thumb in media_thumbnail:
        url = thumb.get('url', '') if isinstance(thumb, dict) else getattr(thumb, 'url', '')
        if url:
            return url

    # â”€â”€ Method 4: Image in description HTML â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    raw = entry.get('summary') or entry.get('description') or entry.get('content', [{}])[0].get('value', '')
    if raw:
        soup = BeautifulSoup(raw, 'lxml')
        img = soup.find('img')
        if img and img.get('src'):
            src = img['src']
            if src.startswith('http'):
                return src

    return None


def get_og_image(url: str) -> str | None:
    """Fetch article page and extract og:image meta tag."""
    try:
        response = requests.get(
            url,
            timeout=PAGE_TIMEOUT,
            headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            },
            allow_redirects=True,
        )

        if response.status_code != 200:
            return None

        soup = BeautifulSoup(response.text, 'lxml')

        # Try og:image first, then twitter:image
        for prop in ('og:image', 'twitter:image'):
            tag = soup.find('meta', property=prop) or soup.find('meta', attrs={'name': prop})
            if tag and tag.get('content'):
                img_url = tag['content'].strip()
                if img_url.startswith('http'):
                    return img_url

    except requests.RequestException:
        pass
    except Exception:
        pass

    return None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  MESSAGE BUILDER
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def build_message(title: str, description: str, link: str) -> str:
    """
    Build clean Persian fashion post.
    No HTML tags used since parse_mode is set to HTML
    but we want plain text with emoji formatting.
    """
    parts = [f"ğŸ’  <b>{title}</b>"]

    if description:
        parts.append(f"\n{description}")

    parts.append("\n")
    parts.append("ğŸ‘— #Ù…Ø¯  âœ¨ #Ø§Ø³ØªØ§ÛŒÙ„  ğŸŒŸ #ØªØ±Ù†Ø¯")
    parts.append("#ÙØ´Ù†_Ø§ÛŒØ±Ø§Ù†ÛŒ  #Ø²ÛŒØ¨Ø§ÛŒÛŒ")
    parts.append(f"\nğŸ”— <a href='{link}'>Ø§Ø¯Ø§Ù…Ù‡ Ù…Ø·Ù„Ø¨</a>")
    parts.append("ğŸ†” @irfashionnews")

    return '\n'.join(parts)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  TELEGRAM SENDER
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def send_to_telegram(
    bot: Bot,
    chat_id: str,
    text: str,
    image_url: str | None
) -> bool:
    """
    Send post to Telegram channel.
    Falls back from photo â†’ text if image fails.
    """
    # â”€â”€ Try sending with photo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if image_url:
        try:
            await bot.send_photo(
                chat_id=chat_id,
                photo=image_url,
                caption=text,
                parse_mode='HTML',
                disable_notification=True,
            )
            return True
        except TelegramError as e:
            print(f"[WARN] Photo send failed ({e}), trying text-only...")

    # â”€â”€ Fallback: text only â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    try:
        await bot.send_message(
            chat_id=chat_id,
            text=text,
            parse_mode='HTML',
            link_preview_options=LinkPreviewOptions(is_disabled=True),
            disable_notification=True,
        )
        return True

    except TelegramError as e:
        print(f"[ERROR] Message send failed: {e}")
        return False


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  ENTRY POINT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if __name__ == "__main__":
    asyncio.run(main())